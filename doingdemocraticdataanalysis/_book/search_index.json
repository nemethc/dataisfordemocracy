[
["index.html", "Doing Democratic Data Analysis Preface", " Doing Democratic Data Analysis Corban Nemeth 2020-05-06 Preface I believe that data, in the hands of public administrators and policy analysts1, has the power to transform the way government works. Big questions will, and should, be asked of big data— the role of government in regulating algorithmic bias, facial recognition, and consumer data privacy is a vital conversation. However, these topics should not detract or deter public administrators and policy analysts from leaning into small data for decision-making purposes. Public administrators and analysts who are data literate will be able to make and inform better decisions while avoiding the pitfalls posed by the latest technological trends. This book represents an opportunity for public administrators and policy analysts to join their subject matter expertise with foundation principles and practices of democratic data analysis— data analysis that is transparent, relevant, and grounded in the context of ethical and effective governance. In this guide, I present an opinionated framework for data analysis in public sector organizations. By opinionated, I mean that I will teach you what I think is the right way to do things given my own experience as a public sector policy and data analyst. Your experience might differ– and that’s great. I hope that where you can use your experience in place of mine, you do to the fullest extent. With that in mind, it is often said that you have to know the rules to break them, so I will teach you the “rules” as I understand them. This is guide is not an excel how-to manual. My hope is that the principles and practices outlined here will allow you to explore whatever analysis tools you are interested in in a democratic manner. With that said, practical examples are given in Excel and in R. Not IT departments↩ "],
["intro.html", "Chapter 1 Introduction 1.1 Principles and Practices 1.2 What is data analysis? 1.3 The Grammar of Data Analysis3", " Chapter 1 Introduction Data analysis is a process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions and supporting decision-making. -*Wikipedia (shame on me) 1.1 Principles and Practices In this handbook, I propose four principles of democratic data analysis. Democratic data analysis is: * Tidy * Reproducible, * Uncertainty-oriented, and * Audience-focused As this isn’t a how-to manual, each chapter will begin with a description of the principles outlined above and arguments for why they are important. This will be followed by practices section where I walk through examples of how to implement these principles in common situations and provide additional materials for you to learn how to apply these principles using common data analysis tools. Why maintain the distinction between principles and practices? Data analysis is driven by the technologies that we have access to. Whether it be the venerable pivot table, or a new-school dashboard platform, or a data-oriented programming language, the principles that I lay out in this handbook supersede specific technologies. Think of it like grammar. You may write by hand, on a computer, using text-to-speech. You may be writing a poem, a novel, an argument, or an instruction manual. But the basic rules of grammar are relevant in whatever medium you choose. Similarly, this guide will teach you the basic “grammar” of democratic data analysis. This will allow you to apply this knowledge in whatever platform or technology you are interested in or have access to. But similar to learning language, it helps to practice. It isn’t much to use to study grammar without ever writing a sentance. The principles section of this guide will include examples in both Excel and R. Government runs on Excel, so all of the examples and exercises will be Excel compatible. If you are comfortable with Excel2 and want to challenge yourself, boost your resume, and become a data superstar, I would highly recommend learning R. 1.2 What is data analysis? Data analysis is the process of transforming numbers on a page to insight into the real world. It’s looking at a table and gaining insight from it. Data analysis can be as simple as adding totals into a column to see cumulative effects, or as complicated as time-series forecasting. But fundamentally, all data analysis is taking inputs and applying those inputs to the real world to gain insight into the real world. It also may be helpful to think about what data analysis isn’t: Data analysis isn’t math. Calculations are great, but a7 + b8 in Excel is deterministic. It gives you one answer. This book is not interested in data analysis that gives you the right answer, because there is no such thing. There are many answers to many questions, depending on how those questions are asked and how the data is analyzed. Data analysis isn’t statistics. This book is about reading and telling the story of your data in a way that can complement expertise and experience to make better decisions. Statistics are often used as a cheap stand-in for domain expertise and are often abused in favor of trusting the analyst or administrator to back up their assumptions with both quantitative and qualitative data. Data analysis isn’t research methods. No set of tools and practices can stand in for asking the right questions, and transforming data into information to answer that question. This book will give you the tools to work with your quantitative data to answer relevant questions, but all good analysis begins with a good question. 1.3 The Grammar of Data Analysis3 As I mentioned before, democratic data analysis has an underlying structure, like a sentance There are rules so these sentences (hopefully) make sense to you, the reader. Similarly, by following common conventions of democratic data analysis, others will be able to “read” your analysis like you are reading this sentence. And also, like grammar, you can break the rules– but it helps to know them first. Here are a couple definitions that will help as you move through this text. Don’t worry about memorizing them, as I will refer back to these definitions frequently. Fields A field is a fancy name for a column. From here on out, every calculation, manipulation, formula, you name it, will be on a column. I want you to forget that you could ever modify a lone cell in Excel. No more formulas in cells. No more typing in values to a cell. Certainly no more writing over data in a cell. Democratic data analysis depends on formulas that work on entire fields. Everything you would need to do to a single cell in Excel can– and should!– be done to an entire column. Variables A variable is something in your data that can change. That’s it! Variables become very important when looking at how to structure your data. Each variable should have its own field. Observations Observations make up the rows of your data set. Each observation should correspond to a specific “thing.” This will make more sense later, I promise. Values Values are the actual data in your table. Each value belongs to 1 (one) observation and 1 (one) variable. Table A table is the grouping of all observations of a similar type. You may already be able to see how these definitions foreshadow some of what is coming in later sections. For example, there are no references to cells. This is intentional. The most important distinction between democratic data analysis and simply working in excel is that in democratic data analysis, (virtually) everything is done on the field level. Changes are made to entire columns, calculations are made on entire columns. Thinking in fields is the first step on the path to democratic data enlightenment. Having data formatted in the structure outlined above forces good data hygiene that will pay massive dividends later on. aka you use vlookup, index(match), pivot tables, or Get &amp; Transform on a somewhat regular basis↩ Adapted from Hadley Wickham’s paper on Tidy Data↩ "],
["tidy-data.html", "Chapter 2 Tidy Data 2.1 The Importance of Tables 2.2 Cleaning vs Tidying 2.3 Pivot Tables and the Meaning of Everything 2.4 Tidy Data- From Wide to Long 2.5 Using lower level data 2.6 Conclusion 2.7 Practices/Resources", " Chapter 2 Tidy Data Tidy data refers to having your data organized in a specific manner suitable for analysis. An obscene amount of time in data analysis is spent getting data into a tidy format. This chapter will walk through common problems and approaches in cleaning and tidying your data, that will make it easier for others to follow and easier for you to work across technologies and subject areas. Keeping your data in a tidy format for analysis will help because it is a fundamentally flexible way of working with data. Keeping scattered, lose data in spreadsheets is a sure way to cause confusion for yourself and others. 2.1 The Importance of Tables If I could convince you of one thing, it would be the value in keeping your data in a table-based excel format. Again, this is true for data analysis purposes. If you are trying to do math or statistics, then the table based framework can sometimes fall short. But if you are concerned with working with data to draw conclusions, then tables are the way to go. 2.2 Cleaning vs Tidying I hate cleaning, but love tidying. Unfortunately, as in with life, one must clean before one tidies. But let’s start with some conceptual definitions. Cleaning refers to the process of scrubbing the data into a way that makes sense to you, the analyst. Oftentimes, and especially in public sector organizations, the data is not clean. Whether you are looking at the output of a SurveyMonkey survey or a canned report that is run from the IT department, your data will come in all shapes and sizes. Cleaning data is the process of transforming data values into ones that make sense for the purposes of your analysis. Here is the first major departure from what you may have been taught about data analysis in Excel. When you get messy data do not change individual cell values (if you can at all help it). Recall from the introductory chapter the difference between cells and fields. Fields, as a reminder, are columns that represent one variable. Whenever possible, use data analysis tools to make changes to the entire field, rather than specific cells. Most data analysis software, outside of Excel, make it difficult or impossible to change individual cell values. This is important for several reasons, most of which we will get to in the next chapter on reproducibility. But for now, thinking in terms of fields, and making changes to entire fields, will save you a lot of work and headache in the long run. Let’s look at a sample dataset that may be similar to one you would encounter in real life. Here is a survey collected by a field manager of a local parks and recreation department on employment. library(tidyverse) library(DT) sites &lt;- tribble( ~&quot;Employee&quot;, ~&quot;Location&quot;, ~&quot;Telecommute?&quot;, ~&quot;Hire Date&quot;, &quot;ron swanson&quot;, &quot;Pawnee City Hall&quot;, &quot;never&quot;, &quot;Unknown&quot;, &quot;Knope, Leslie&quot;, &quot;Field Duty&quot;, &quot;1 day/week&quot;, &quot;2011-6-1&quot;, &quot;Andy Dwyer&quot;, &quot;sullivan street pit&quot;, &quot;40 hours&quot;, &quot;March 1, 2013&quot;, &quot;Jerry Gergich&quot;, &quot;City Hall&quot;, &quot;never&quot;, &quot;6/1/1985&quot;, &quot;Garry Gergich&quot;, &quot;City Hall&quot;, &quot;never&quot;, &quot;6/1/1985&quot;, &quot;ben wyatt&quot;, &quot;Partridge, Minnesota&quot;,&quot;&quot; , &quot;Jan. 1, 2010&quot; ) sites %&gt;% datatable( extensions = &#39;Buttons&#39;, options = list(dom = &#39;Bfrtip&#39;, buttons = &#39;excel&#39;, searching = FALSE)) In this example, it would be trivial to go in to the Excel file and clean up the dates, names, and locations by hand. However, you could imagine this survey replicated for a department of forty employees. It quickly becomes unfeasable to make those edits by hand. When this is the case, there are functions in Excel and R that will make your life much easier. Here is annotated code for how I would go about cleaning this table in R. The friendly syntax of the tidyverse packages makes it easy to follow along, even if you aren’t comfortable writing it yourself. You can accomplish all these similar transformations using Get &amp; Transform in Excel. #creating a new table called &quot;sites_cleaned&quot;, starting with the old table &quot;sites&quot; sites_cleaned &lt;- sites %&gt;% #switching the order of names that are backwards mutate(Employee = if_else(Employee == &quot;Knope, Leslie&quot;, &quot;Leslie Knope&quot;, Employee)) %&gt;% #seperate employee names into two columns separate(Employee, into = c(&quot;first_name&quot;, &quot;last_name&quot;)) %&gt;% #renaming column names to standard format rename(location = Location, telecommute_hours =`Telecommute?`, hire_date = `Hire Date`) %&gt;% #changing first and last names and locations to Title Case mutate(first_name = str_to_title(first_name), last_name = str_to_title(last_name), location = str_to_title(location)) %&gt;% #coding location data to three categories, In Office, In Field, or Other mutate(location = case_when( str_detect(location, &quot;City Hall&quot;) ~ &quot;In Office&quot;, str_detect(location, &quot;Field&quot;) ~ &quot;In Field&quot;, str_detect(location, &quot;Street&quot;) ~ &quot;In Field&quot;, TRUE ~ &quot;Other&quot;), #coding hours to numeric telecommute_hours = case_when( telecommute_hours == &quot;never&quot; ~ 0, telecommute_hours == &quot;1 day/week&quot; ~ 8, telecommute_hours == &quot;40 hours&quot; ~ 40 ) ) #print to datatable sites_cleaned%&gt;% datatable( extensions = &#39;Buttons&#39;, options = list(dom = &#39;Bfrtip&#39;, buttons = &#39;excel&#39;, searching = FALSE)) This may seem like a lot of work for a small table. But as your data grows, it is much easier to operate on entire fields at a time. This is especially true in Get &amp; Transform, which makes it very easy to transform and clean data using all the same steps as the R code above. As you can see, our table is now “cleaned” and organized consistently. 2.3 Pivot Tables and the Meaning of Everything The Ultimate Answer to Life, The Universe and Everything is… the pivot table -Dougas Adams/Corban Nemeth Pivot tables are the world’s most common, most helpful, and most underrated data analysis tool. If you understand the mechanics of the pivot table, you have everything you need to be a data analysis expert. PowerBI or Tableau interactive charts and graphs are just pivot tables in disguise. Understanding what is needed to make a pivot table work is the key to the wide world of data analysis. What is so important about pivot tables? First and foremost, pivot tables force you to think in terms of fields, not in terms of cells. In order for a pivot table to be effective, the data has to be organized in a table. And there is a right and a wrong way to putting data in a table. If your pivot table is not working properly, it is likely because your data isn’t tidy. A pivot table groups data by field and allows the user to drag fields to the rows or columns of the pivot table. This is effective when each field is a variable (something that can change), and each row is a seperate observation of some phenomena of interest. In short, pivot tables depend on tidy data. Tidy data is the way your data should be organized before you begin your analysis. In tidy data, each column is a variable, each row is an observation, and each table is an associated set of observations. What does that mean in practice? Consider the following example. Below is a table4 that shows types of retirement visits for a month at a state’s Department of Retirement Services by the employee who took the visit and the visit type. #build sample data table visits &lt;- tribble( ~&quot;Employee&quot;, ~&quot;Phone Visits&quot;, ~&quot;Office Visits&quot;, ~&quot;Online Visits&quot;, &quot;Danielle&quot;, 6, 11, 23, &quot;Ramona&quot;, 11, 5, 18, &quot;Ross&quot;, 10, 10, 10 ) #print to datatable visits%&gt;% datatable( extensions = &#39;Buttons&#39;, options = list(dom = &#39;Bfrtip&#39;, buttons = &#39;excel&#39;, searching = FALSE), caption = &quot;Visits to the Dept. of Retirement Services in a given month by employee&quot; ) Data are frequently displayed in this “wide” format. It works great for presentation, but not great for data analysis. The shortcomings of data in this format may become apparent when you attempt to work with the data in a pivot table. This is becuase our columns aren’t truly variables. Remember, variables are elements of an observation that can change. You can drag the fields from the top row to the grey box below, for columns, and the left, for rows. This becomes unmanegable quickly. rpivotTable::rpivotTable(visits, width = &quot;60%&quot;, height = &quot;60%&quot;) 2.4 Tidy Data- From Wide to Long Let’s apply our criteria of tidy data to this set: Variables At first glance, it doensn’t look like this is a problem. But think again. Is phone visits really a variable? Or is the real variable of interest number of visits? And are our column names are actually variables too (type of visit)? Let’s take another swing at setting up our table for data analysis purposes. This can be accomplished easily in R using the code below, or in Excel by loading the data with Get and Transform -&gt; selecting the three “visits” columns -&gt; right clicking -&gt; and selecting “unpivot columns.” #We have already loaded the &quot;tidyverse&quot; library so we do not have to do it again #we are editing the &quot;visits&quot; table already created by storing it in a new table pivot_visits pivot_visits &lt;- visits %&gt;% #using pivot_longer on every column except &quot;employee&quot; and setting the name of the new columns pivot_longer(-Employee, names_to = &quot;Visit Type&quot;, values_to = &quot;Number of Visits&quot;) knitr::kable(pivot_visits, caption = &quot;Visits to the Dept. of Retirement Services in a given month&quot;) Table 2.1: Visits to the Dept. of Retirement Services in a given month Employee Visit Type Number of Visits Danielle Phone Visits 6 Danielle Office Visits 11 Danielle Online Visits 23 Ramona Phone Visits 11 Ramona Office Visits 5 Ramona Online Visits 18 Ross Phone Visits 10 Ross Office Visits 10 Ross Online Visits 10 Now this is a table that is much easier to analyze in an Excel pivot table or with a variety of R functions. However, it does look worse and is less intuitive for human readers. Thankfully, using data in this format, it is easy to recreate the original table for presentation, while also giving a variety of options for formatting and plotting. Use the pivot table below to recreate the original table using the tidy data. *Hint- Instead of Count, select Sum -&gt; Number of Visits as the value field. It is far easier to work with fields when they are in a tidy format. rpivotTable::rpivotTable(pivot_visits, width = &quot;60%&quot;, height = &quot;400px&quot;) It is also easier to do a variety of calculations on the data now that is in a “tidy” format. For example, creating descriptive statistics accross groups is very straightforward. desc_visits &lt;- pivot_visits %&gt;% group_by(`Visit Type`) %&gt;% summarise(`Avg Visits` = mean(`Number of Visits`)) 2.5 Using lower level data Let’s introduce a slightly more complicated tidy data problem, using the same base data as before. visits_retirements &lt;- tribble( ~&quot;Employee&quot;, ~&quot;Phone Visits&quot;, ~&quot;Phone Retirements&quot;, ~&quot;Office Visits&quot;, ~&quot;Office Retirements&quot;, ~&quot;Online Visits&quot;, ~&quot;Online Retirements&quot;, &quot;Danielle&quot;, 6, 4, 11, 8, 23, 15, &quot;Ramona&quot;, 11, 7, 5, 3, 18, 15, &quot;Ross&quot;, 10, 8, 10, 7, 10, 9 ) knitr::kable(visits_retirements, caption = &quot;Visits to the Dept. of Retirement Services in a given month by employee and associated client retirements&quot;) Table 2.2: Visits to the Dept. of Retirement Services in a given month by employee and associated client retirements Employee Phone Visits Phone Retirements Office Visits Office Retirements Online Visits Online Retirements Danielle 6 4 11 8 23 15 Ramona 11 7 5 3 18 15 Ross 10 8 10 7 10 9 Hopefully you will see a similar pattern here. Now, there are three variables: Visit type, number of visits, and number of retirements. Again, this data works fine for presentation but could use tidying to ease in analysis. visits_retirements %&gt;% DT::datatable( extensions = &#39;Buttons&#39;, options = list(dom = &#39;Bfrtip&#39;, buttons = &#39;excel&#39;, searching = FALSE)) Try to tidy this in R or Excel Get and Transform. See this footnote5 or look at the code if you need a hint. visits_retirements_tidy &lt;- visits_retirements %&gt;% pivot_longer(cols = -Employee, names_to = c(&quot;Visit Location&quot;, &quot;Type&quot;), names_sep = &quot; &quot;) print(visits_retirements_tidy) ## # A tibble: 18 x 4 ## Employee `Visit Location` Type value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Danielle Phone Visits 6 ## 2 Danielle Phone Retirements 4 ## 3 Danielle Office Visits 11 ## 4 Danielle Office Retirements 8 ## 5 Danielle Online Visits 23 ## 6 Danielle Online Retirements 15 ## 7 Ramona Phone Visits 11 ## 8 Ramona Phone Retirements 7 ## 9 Ramona Office Visits 5 ## 10 Ramona Office Retirements 3 ## 11 Ramona Online Visits 18 ## 12 Ramona Online Retirements 15 ## 13 Ross Phone Visits 10 ## 14 Ross Phone Retirements 8 ## 15 Ross Office Visits 10 ## 16 Ross Office Retirements 7 ## 17 Ross Online Visits 10 ## 18 Ross Online Retirements 9 In this case, we actually pivoted too far. It will probably be more useful to have the counts of visits and retirements in their own category. Keep in mind the scope of the observation– It is perfectly valid for each to have their own column, as it is visits and retirements per month. visits_retirements_tidy2 &lt;- visits_retirements_tidy %&gt;% pivot_wider(id_cols = c(Employee, `Visit Location`, Type), names_from = Type, values_from = value) print(visits_retirements_tidy2) ## # A tibble: 9 x 4 ## Employee `Visit Location` Visits Retirements ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Danielle Phone 6 4 ## 2 Danielle Office 11 8 ## 3 Danielle Online 23 15 ## 4 Ramona Phone 11 7 ## 5 Ramona Office 5 3 ## 6 Ramona Online 18 15 ## 7 Ross Phone 10 8 ## 8 Ross Office 10 7 ## 9 Ross Online 10 9 From here, it is easy to do calculations based on fields, rather than cells. For example, in R or Get and Transform, you could add the following: visits_pct &lt;- visits_retirements_tidy2 %&gt;% mutate(pct_retirements = Retirements / Visits) print(visits_pct) ## # A tibble: 9 x 5 ## Employee `Visit Location` Visits Retirements pct_retirements ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Danielle Phone 6 4 0.667 ## 2 Danielle Office 11 8 0.727 ## 3 Danielle Online 23 15 0.652 ## 4 Ramona Phone 11 7 0.636 ## 5 Ramona Office 5 3 0.6 ## 6 Ramona Online 18 15 0.833 ## 7 Ross Phone 10 8 0.8 ## 8 Ross Office 10 7 0.7 ## 9 Ross Online 10 9 0.9 And then, one of the most useful things you can do is develop formulas by grouping of rows. For example, you may want to know the total number of visits and retirements by retiree, regardless of visit location. That can be accomplished in a pivot table. 2.6 Conclusion 2.7 Practices/Resources Data was created for demonstration purposes↩ powerquery hints↩ "],
["reproducible-analysis.html", "Chapter 3 Reproducible Analysis 3.1 Do It For Your Future Self 3.2 Comment Everything 3.3 Give Yourself Credit 3.4 Version Control 3.5 Practices", " Chapter 3 Reproducible Analysis 3.1 Do It For Your Future Self Many things take more time to do up front, but save you from massive headaches down the road. Brushing your teeth. Oil changes. Preventative maintenence is the name of the game. The same thing applies in democratic data analysis. Learning how to brush the teeth of your analysis will pay massive dividends down the road, as someone else (or you, more likely), need to go back through and understand, replicate, or validate your findings. The second principle of democratic data analysis is reproducability. By this, I mean anything that makes it easy to look at your analysis and understand what is going on. This is where classic data analysis in Excel falls short. I believe it is almost a universal experience in the public sector to recieve a workbook full of broken links, formulas pointing in every direction, and no sense of where the original data is or what has happened to it since. In thinking about creating reproducible data analysis, it is important to keep in mind that data analasys should be structured from beginning to end, like a story. In the beginning, there is raw data that you pulled from a report, compiled yourself, or otherwise recieved. In Act 1, you use the practices we learned in the previous section to make the raw data tidy– without distroying the original data. You should use tools that allow to to non-destructively manipulate and iterate on your data. Both Get &amp; Transform and R allow you to do this by default. In Act 2, which will be the next chapter, you use your data to create a picture of the world before you share it with others in the final Act 3. The practices of reproducability that you will use here apply throughout the other chapters. It may seem like a waste of time, but if you have ever come back to a complicated excel workbook after spending even days away, this will make your life much easier. 3.1.1 Do Not Destroy As I mentioned before, the existential dread that occurs when opening someone else’s workbook and immediatly recieving broken links, color-coding6, and a spiderweb of formulas may be a universal experience in the public sector. But there is a better way to do things. Reproducible analysis is linear. It progresses in a certain direction– from data load to final analysis. Things happen discretely. The blessing and curse of spreadsheets is that they are unbooud by time. There is no natural direction, just a sea of little boxes spreading out as far as the eye can see7. However, there are ways to impose a linear structure to your analysis. The first thing I want to emphasize is PLEASE DO NOT DESTROY, ALTER, OR MANIPULATE YOUR UNDERLYING DATA. Your underlying data is like the foundation of your house. Democratic data analysis starts with a foundation of data, and builds on top of it. Oftentimes, it seems easier to simply click and drag cells around in an excel workbook, changing values here and there as you see fit. This may work in small use cases, but what if you have another idea? Or come up with a different question, where your data needs to be coded differently? Reproducible analysis makes it substantially easier to revise and rewrite after the fact. If you were writing a well-sourced research article, you wouldn’t delete your notes and references for the material that didn’t make it into your final product. In the same way, keeping record of the changes that you make to your analysis will pay dividends when your approach changes. 3.1.2 Red flags for reproduciblitly I’ll start with a list of things you want to avoid in the pursuit of reproducible democratic data analysis 3.1.2.1 Copy and Paste If you find yourself copying and pasting values in an excel workbook, you are not engaging in reproducible analsys– full stop. Copy and paste (or worse, cut and paste) doesn’t leave breadcrumbs for you or anyone who may come after you. It is incredibly difficult to follow the trail of an analysis built on top of copy-paste. 3.1.2.2 Repeating Yourself There is an old adage in programming - Don’t Repeat Yourself. DRY. Keeping your data analysis DRY is a good habit to get in to. If you find yourself repeating the same task more than three times, chances are there is a better, more programamatic way to go about what you are doing. What do I mean by repeating yourself? This would be going through every row of an 100-row table to add (or remove)a space between words, capitalizing letters, doing specific calcualtions. All of these tasks can be easily automated using virtually all data analysis tools. This not only saves you time, but makes it easier for your reader to see how the data has changed in the course of your analysis. To preview the Practices section, Get and Transform allows you to make flexible value transformations on entire fields at a time. This reduces the need for repetitive data cleaning. And if you are already taking my advice and no longer editing individual data cells, you will have a much easier time avoiding repeating yourself. 3.2 Comment Everything Comments are wonderful. They are notes to yourself that you should leave at almost every step of your analysis. I frequently do not leave comments. Never have I come back to an uncommented data transformation and been happy with my past self. At worst, leaving comments takes a couple seconds of your time you will never get back. At best, it saves you or your organization from a massive headache when you are able to catch your own errors or update your analysis easier in the future. 3.3 Give Yourself Credit Don’t hard code Excel values in cells, etc. This is easier to do when you use the tools shown in the practices section. 3.4 Version Control 3.4.1 Save as 3.4.2 Git/GitHub 3.5 Practices The value of Get &amp; Transform Data is that it forces you to build iteratively on top of your data foundation. It also convineiently records each step along the way. You even have the ability to save comments right there in the query editor. This is a remarkly easy and intuitive way to build a data tranformation pipeline that will be valuable know and into the future. for the love of democracy, PLEASE do not color code your data↩ This is where programming languages such as R have an inherent advantage. Code runs in order, from first to last↩ "],
["uncertainty-oriented-analysis.html", "Chapter 4 Uncertainty Oriented Analysis 4.1 Embracing Uncertainty for Better Outcomes 4.2 Why Model? 4.3 Example two 4.4 Use Assumptions, and Document Them 4.5 Don’t get out over your skis", " Chapter 4 Uncertainty Oriented Analysis All models are wrong, but some are useful – Gearge Box 4.1 Embracing Uncertainty for Better Outcomes What is the point of data analysis? Often, it is to use data to summarize the world around you. In a sense, all data analysis is model building, and by definition, a model is a simplified version of the world. Any time the analyst is using data analysis to inform decision making, she is in a sense making a model. Models are commonly thought of to provide answers to the question at hand. Model buidling, and data analysis more generally, never gives you “the” answer. Data analysis only gives you a answer, if it gives you an answer at all. If you have been following along to this point, you have learned the value of data analysis that is structured and built up, not out. Frequently, data analysis is valuable because it tells you what isn’t the answer. Knowing how to build uncertainty into your analysis is incredibly valuble as analysis – models– are used to implement important policies that impact the public good. Incorporating uncertainty into your analysis will make you both more credible and force you to use your subject matter expertise in addition to your data skills. Any data analysis that simplifies the world (hint– that’s all of it) can be considered a model. So I will use those phrases interchangeablly in this section. 4.2 Why Model? Models transform data into decision making. Models are useful exactly because they are wrong! Because models are wrong, we can critically examine the 4.3 Example two electoral 2016 map distribution of possible results 4.4 Use Assumptions, and Document Them (create dummy model regarding time saved through learning democratic data practices) 4.5 Don’t get out over your skis ##Practices Talk about modeling implies math or statistics, when it really doesn’t. All a model seeks to do is to simply phenomena in a manner that can be comprehended and used to make decisions in conjunction with subject matter expertise. "],
["audience-focused.html", "Chapter 5 Audience-Focused 5.1 Show and Tell", " Chapter 5 Audience-Focused 5.1 Show and Tell ##Vizualizaiton is anything that presents your evidence– think critically about it! "],
["applications.html", "Chapter 6 Applications 6.1 Tying it all together", " Chapter 6 Applications 6.1 Tying it all together "]
]
